{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyennguyen/anaconda3/envs/research/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nguyennguyen/anaconda3/envs/research/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/nguyennguyen/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 101MB/s] \n"
     ]
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/nguyennguyen/Documents/egotopo/research/P01/rgb_frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(eog:53496): Gtk-WARNING **: 23:10:47.598: cannot open display: \n"
     ]
    }
   ],
   "source": [
    "input_image = Image.open(\"/home/nguyennguyen/Documents/egotopo/research/P01/rgb_frames/frame_0000000012.jpg\")\n",
    "img.show()\n",
    "data = np.asarray(input_image)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /home/nguyennguyen/.cache/torch/hub/v0.10.0.zip\n",
      "/home/nguyennguyen/anaconda3/envs/research/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nguyennguyen/anaconda3/envs/research/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = preprocess(input_image)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.7835e-01, -6.2027e-01, -1.1462e+00, -6.4771e-02,  1.4664e+00,\n",
      "         5.2264e-01,  1.0016e+00, -6.4922e-01, -1.2056e+00, -5.2284e+00,\n",
      "        -3.2616e+00, -3.8592e+00, -1.8689e+00, -3.3769e+00, -4.1787e+00,\n",
      "        -3.8548e+00, -2.7644e+00, -3.8361e+00, -4.2662e+00, -3.1415e+00,\n",
      "        -4.5216e+00, -5.5762e+00, -5.7689e+00, -3.0599e+00, -6.8103e+00,\n",
      "        -1.2627e-01, -1.4698e+00, -1.2028e+00,  1.1628e+00,  1.1359e+00,\n",
      "        -1.0309e+00, -1.6405e+00, -7.9683e-01, -2.2350e-01,  9.0946e-02,\n",
      "        -3.6212e-01, -9.5016e-01, -5.3325e-01, -2.0936e+00, -2.9304e+00,\n",
      "        -3.2433e+00, -2.9510e+00, -3.7609e+00, -1.5447e+00, -1.6571e+00,\n",
      "        -9.5592e-01, -3.3239e+00, -2.2543e+00, -2.1181e+00, -1.9445e+00,\n",
      "        -2.8618e+00,  3.5864e+00, -9.7451e-01, -1.8248e+00, -1.7438e+00,\n",
      "        -3.8436e+00, -1.6931e+00, -2.6342e+00, -2.6206e+00, -2.4895e+00,\n",
      "        -1.3751e-01, -4.4997e-01, -1.2767e+00, -8.2449e-01, -1.4502e+00,\n",
      "        -2.0425e+00, -9.7084e-01, -1.7683e+00, -2.2521e+00, -2.4671e+00,\n",
      "        -3.8255e+00,  1.1170e+00, -3.8068e+00, -1.3946e+00, -3.1728e+00,\n",
      "        -4.7688e-01, -2.2229e-02, -1.9047e+00,  1.6579e+00, -1.9169e-01,\n",
      "        -2.8637e+00, -4.4480e+00, -3.3847e+00, -3.4204e+00, -4.7518e+00,\n",
      "        -2.9366e+00, -2.9777e+00, -1.4436e+00, -3.2102e+00, -1.2426e+00,\n",
      "        -4.3721e+00, -3.7001e+00, -3.4746e+00, -3.0710e+00, -4.0465e+00,\n",
      "        -4.9282e+00, -1.9046e+00, -2.8007e+00, -5.3514e+00, -2.1545e+00,\n",
      "        -4.7972e+00, -2.1321e+00,  3.4789e-01,  7.1673e-01, -2.7819e-01,\n",
      "        -1.5477e+00, -3.9074e-01, -9.0590e-01, -6.4954e-01, -3.6446e+00,\n",
      "        -3.1693e+00, -2.3142e+00,  2.4466e+00,  1.0613e+00,  5.0486e-01,\n",
      "        -1.8372e+00, -2.9496e+00,  2.3146e-02,  7.4378e-01, -2.2948e+00,\n",
      "        -2.1818e+00, -2.9064e-02,  4.1493e-01, -2.2300e+00, -3.1090e-01,\n",
      "         1.1354e+00, -1.0526e-01, -5.6777e+00, -6.9761e+00, -4.1624e+00,\n",
      "        -3.4627e+00, -6.3276e+00, -4.9209e+00, -6.5589e+00, -4.8269e+00,\n",
      "        -4.7465e+00, -6.1227e+00, -3.6256e+00, -5.9411e+00, -5.4217e+00,\n",
      "        -4.9227e+00, -6.4500e+00, -6.4748e+00, -5.1791e+00, -2.2645e+00,\n",
      "        -1.9005e+00, -3.3760e+00, -5.8978e+00, -2.2665e+00, -1.2581e+00,\n",
      "        -5.0063e-01,  2.4202e+00, -1.8877e+00,  2.2956e-01,  1.5721e+00,\n",
      "         2.6925e-01,  1.0714e+00, -7.1982e-01,  1.7972e-01,  1.5008e+00,\n",
      "        -5.4435e-01,  2.2171e+00,  1.8621e+00,  2.6286e+00, -1.3746e+00,\n",
      "        -7.4826e-01, -1.2886e+00, -8.6133e-01,  2.2206e+00, -5.0132e-01,\n",
      "        -7.7351e-01,  1.6558e+00,  1.0390e+00, -5.5546e-01,  6.3508e-01,\n",
      "        -1.8600e+00, -1.1730e+00, -2.0922e+00,  1.2888e+00,  1.9085e+00,\n",
      "         1.4223e+00, -1.6998e+00,  7.6111e-01, -1.8997e+00,  1.9398e+00,\n",
      "         2.2251e+00,  1.2857e+00, -5.9130e-01, -1.4691e+00,  1.0011e+00,\n",
      "        -2.6877e-01,  1.8399e-01,  9.1496e-01,  1.2389e-01, -4.5483e-02,\n",
      "         1.2004e+00, -3.7258e-01,  1.9951e-01, -1.1044e+00,  1.7380e+00,\n",
      "        -5.2637e-01,  7.8859e-01,  1.4655e+00,  1.7103e+00,  1.4619e+00,\n",
      "        -8.5742e-01, -1.2725e+00,  2.7909e+00,  3.3696e+00,  2.8810e+00,\n",
      "        -3.3416e-01,  1.1428e+00, -6.0843e-01,  4.4698e-01, -1.2128e+00,\n",
      "        -2.2591e-02, -3.0565e-02, -1.2942e-01, -1.7770e-01,  1.0834e+00,\n",
      "         1.1769e+00, -1.5860e-01,  9.5236e-01,  4.0180e+00,  1.6085e-01,\n",
      "         1.5015e+00, -1.6263e-01,  2.8790e+00, -1.6555e-01,  1.0967e+00,\n",
      "         1.2906e+00, -8.1979e-02, -1.1385e+00, -7.3133e-01,  1.2352e+00,\n",
      "         6.7336e-01,  1.5438e+00,  1.3826e+00, -1.9738e-01,  1.2285e+00,\n",
      "        -4.9796e-01,  3.0276e-01,  1.5779e+00,  1.4448e+00, -1.1622e+00,\n",
      "         2.7980e+00, -4.2584e-02,  6.1094e-01,  5.3427e-01,  1.5280e+00,\n",
      "         1.7444e+00,  1.4581e+00, -6.1437e-01,  1.0013e+00,  1.1015e+00,\n",
      "        -1.8001e+00,  5.9592e-01,  2.1495e+00,  2.6085e+00,  2.9497e+00,\n",
      "         2.3256e+00, -1.9764e-01,  4.2489e-01,  3.2290e+00,  2.5282e+00,\n",
      "         9.4399e-01,  3.2074e-02, -1.1768e-01, -2.2369e-01, -3.2329e+00,\n",
      "        -1.8836e+00, -1.9420e+00, -4.0265e+00,  1.7135e+00, -2.0941e+00,\n",
      "        -2.8993e+00, -1.9410e+00, -1.9267e+00, -2.9439e+00, -1.9080e+00,\n",
      "        -2.4700e+00, -1.3178e-01, -1.1796e+00,  9.3609e-01,  3.1385e+00,\n",
      "        -1.4280e-01, -8.8968e-01, -1.7687e+00, -3.7155e+00, -5.0462e+00,\n",
      "        -3.1090e+00, -2.4657e+00, -2.5785e+00, -4.0078e+00, -3.0534e+00,\n",
      "        -4.2728e+00, -1.8693e+00, -3.0501e+00, -1.2969e+00, -9.0143e-01,\n",
      "        -3.0241e+00, -3.7741e-01,  1.4040e-01, -2.8075e+00,  2.3222e-01,\n",
      "         1.6102e+00,  1.2068e+00, -2.1996e+00, -3.3346e+00, -1.8171e+00,\n",
      "        -6.9457e-01, -1.0867e+00, -6.4548e-02, -2.6290e+00,  3.2865e+00,\n",
      "        -1.9285e+00, -4.1423e+00, -3.8643e+00, -3.4951e+00, -4.6398e+00,\n",
      "        -4.0635e+00, -3.6355e+00, -2.6300e+00, -2.6343e+00, -2.3556e+00,\n",
      "        -1.3915e+00, -2.4525e+00, -1.4505e+00, -2.1487e+00, -3.0918e+00,\n",
      "        -1.0286e+00, -2.8994e+00,  1.5723e+00, -1.0318e+00, -1.3390e+00,\n",
      "        -5.0631e+00, -3.4554e+00, -1.9403e+00,  4.5925e-01, -7.8418e-01,\n",
      "        -3.6197e+00,  8.5388e-01, -8.0585e-01, -9.9118e-01, -1.8248e+00,\n",
      "         2.6898e+00,  1.8523e-01,  5.4550e-01, -6.1787e-01, -1.0196e+00,\n",
      "        -8.0422e-01, -6.4661e-01, -1.2019e+00, -1.3032e+00, -3.1629e-01,\n",
      "        -1.3846e+00, -3.5402e-01, -2.3724e+00, -3.3883e-01,  8.1467e-01,\n",
      "        -2.4201e+00,  1.8625e+00,  7.6021e-01,  9.6285e-01, -4.6472e+00,\n",
      "        -3.1361e+00, -4.1664e+00, -4.4922e+00, -3.6486e+00, -5.4117e+00,\n",
      "        -3.8072e+00, -2.6829e+00, -3.2759e+00, -1.0231e+00, -1.2001e+00,\n",
      "        -4.7084e+00, -3.1779e+00, -3.2970e+00, -1.6925e+00, -3.6288e+00,\n",
      "        -1.7298e+00, -3.7284e+00, -2.4994e+00, -2.3311e+00, -2.2234e+00,\n",
      "        -1.4183e+00, -1.0958e+00, -6.5142e-01, -5.3415e-01, -2.6319e+00,\n",
      "        -2.7501e+00, -1.5427e+00, -2.5045e+00, -7.7156e-01,  7.8475e-02,\n",
      "        -2.7941e+00, -2.5756e+00, -3.2287e+00,  1.0099e+00, -5.1086e-01,\n",
      "        -8.1626e-01,  1.1409e+00,  3.5786e+00, -8.3481e-03, -1.6649e-01,\n",
      "         1.4304e+00,  5.8349e-01,  2.7314e+00,  1.6849e+00,  1.6775e-02,\n",
      "        -2.8654e-01, -5.5578e-01,  3.1760e+00,  1.7892e+00, -3.7319e-01,\n",
      "         1.7133e+00, -4.6666e-01,  2.7054e-01, -2.8899e+00,  1.0767e+00,\n",
      "         3.1659e+00,  1.4395e+00,  3.2311e+00,  3.5338e+00,  2.7188e-01,\n",
      "        -1.7786e+00,  8.9861e-01,  3.6578e+00,  3.6725e+00,  6.6317e-01,\n",
      "        -1.2464e+00,  1.4496e+00,  2.5789e-01,  5.2540e-01, -1.8343e+00,\n",
      "         4.4531e+00,  2.6873e+00,  8.6694e-01, -2.0525e+00, -3.9851e-01,\n",
      "         1.0957e+00,  2.5808e-01,  1.3809e+00, -3.0519e+00, -2.2253e-01,\n",
      "        -4.5452e-01, -1.2582e+00,  9.8841e-01,  8.7187e-01, -2.0932e-01,\n",
      "         2.4366e+00, -1.8809e+00, -8.8494e-02,  1.5499e+00, -1.1162e+00,\n",
      "         6.6612e-02,  2.8862e-01, -1.2016e+00, -2.7775e+00, -5.1505e-01,\n",
      "        -1.0991e+00,  8.7138e-01,  7.6688e-01,  1.1556e+00, -1.3416e+00,\n",
      "        -2.4633e-01,  1.0821e+00,  1.5159e+00,  2.0182e+00,  4.6843e+00,\n",
      "         2.5454e+00,  3.7840e+00, -1.2435e-01,  2.8977e+00, -1.9235e+00,\n",
      "        -1.8062e+00,  2.4607e+00, -4.9911e-01,  3.1046e+00,  1.9429e+00,\n",
      "        -3.6471e-01, -6.9457e-01, -1.9143e-01, -4.2024e-01, -1.7213e+00,\n",
      "         1.0452e+00,  4.9504e+00, -1.7503e+00, -1.4355e-01, -2.1067e+00,\n",
      "        -1.1688e+00,  2.0028e+00,  3.8662e+00,  2.6652e+00,  1.7039e+00,\n",
      "         2.0090e+00, -4.1989e-02,  5.6295e-01,  9.7477e-01,  3.6786e+00,\n",
      "         7.2259e-01, -8.2317e-01,  4.5521e+00, -1.2788e+00,  1.0426e+00,\n",
      "         3.0904e+00,  1.4265e+00,  5.0651e-01,  1.9287e+00, -3.4862e-01,\n",
      "        -1.8582e+00,  1.3358e+00,  2.3687e+00,  4.4973e-01,  1.9127e+00,\n",
      "         2.6381e+00,  4.2904e+00,  1.7516e+00,  2.6094e+00,  3.0778e+00,\n",
      "         1.4720e+00,  4.4491e+00, -1.3326e+00,  2.1054e+00,  8.9439e-01,\n",
      "         7.4099e-01,  4.8658e+00,  1.7275e+00,  2.4657e+00, -1.2049e+00,\n",
      "         7.5301e-01,  1.3267e-01,  6.1809e+00, -2.5985e+00,  3.9384e+00,\n",
      "         2.8389e+00, -7.7513e-01, -2.3847e+00,  1.5081e+00,  1.0749e+00,\n",
      "         5.1814e-01,  4.2024e+00,  2.0567e+00,  2.9455e+00,  2.0986e+00,\n",
      "         2.1462e+00,  3.3396e+00, -1.4566e+00, -4.9856e-01, -1.2220e+00,\n",
      "         1.8339e+00, -1.9139e+00,  1.3347e+00,  2.9483e+00, -2.9158e+00,\n",
      "         1.6868e-01,  2.3062e+00, -1.5763e+00,  1.0770e+00,  3.0747e+00,\n",
      "        -2.2768e-01,  2.0335e+00, -7.2758e-01, -2.2315e+00,  1.3650e+00,\n",
      "        -4.6925e-01, -4.7120e-01,  2.4741e+00, -6.5427e-01,  1.9870e+00,\n",
      "         1.5650e+00,  7.0633e-02, -7.1248e-01, -8.9534e-01, -9.1340e-01,\n",
      "         4.0733e+00, -1.5501e+00,  2.4170e+00,  8.1187e-02,  5.0035e+00,\n",
      "         9.0897e-01, -1.1176e-01,  1.3423e+00,  2.3635e+00, -7.2380e-01,\n",
      "        -1.5589e+00,  3.1032e+00,  2.7379e+00,  1.0141e+00,  2.7583e+00,\n",
      "        -1.8126e+00, -7.9787e-01,  1.5441e-02, -1.1371e-01,  2.6936e+00,\n",
      "         9.1944e-01,  4.3458e-01,  1.4594e+00,  2.7152e+00,  1.9585e-01,\n",
      "         6.7862e-01, -7.3820e-01, -2.5103e+00,  7.6766e-01,  1.0512e+00,\n",
      "        -1.0802e+00,  3.2504e+00,  2.3998e+00, -1.1896e+00,  3.8902e+00,\n",
      "        -1.5244e+00,  2.4738e+00, -1.9591e+00,  2.4204e+00, -4.9855e-01,\n",
      "         1.6654e+00, -1.9450e+00,  7.5370e-01,  3.1959e+00,  1.7563e+00,\n",
      "         2.4853e+00,  2.6755e+00, -1.4239e+00, -1.6438e+00,  1.9136e+00,\n",
      "        -1.0331e+00, -2.5054e-01,  2.7224e+00, -2.0899e-02, -9.4184e-01,\n",
      "         2.2595e+00, -9.0243e-01,  1.3304e+00, -2.1736e-01,  3.1365e+00,\n",
      "         1.7576e-01, -1.0136e+00,  8.6872e-01, -3.4416e-01, -6.4648e-01,\n",
      "        -1.3584e+00,  2.1015e+00,  3.6462e+00,  2.3737e-01,  1.9244e-01,\n",
      "        -2.7584e+00,  1.8260e+00,  4.4004e-01,  1.3098e+00, -2.0384e+00,\n",
      "        -9.2214e-01,  2.4495e+00,  9.2486e-01,  4.1455e+00,  2.8648e+00,\n",
      "         6.6123e-01,  8.7839e-01,  3.0938e+00, -2.3698e+00, -8.9517e-01,\n",
      "         1.1424e+00,  1.8434e+00,  5.9166e-01,  2.7561e-01,  2.9764e+00,\n",
      "         4.3921e+00,  3.2175e+00,  3.9760e-01,  1.1917e+00,  1.8251e-01,\n",
      "         4.9080e+00, -7.3549e-02,  7.2219e-01,  1.3855e+00,  4.2693e+00,\n",
      "         3.5391e+00,  4.8054e-01, -2.3460e-01,  1.6424e+00, -1.4168e+00,\n",
      "        -4.0923e-02,  1.4481e+00,  7.1303e-01,  1.0514e+00,  1.0269e+00,\n",
      "        -2.3153e+00, -1.8661e-01,  1.7510e+00,  6.4934e-01, -1.2549e+00,\n",
      "         2.8370e+00,  1.3158e+00, -1.0876e+00, -5.5569e-01, -1.4054e+00,\n",
      "         4.7005e-01,  1.1753e+00,  1.2521e+00,  1.1954e+00,  1.2543e+00,\n",
      "         3.4949e+00, -1.8492e+00, -2.1281e+00,  7.9779e-01,  1.1919e-01,\n",
      "        -1.0026e-01,  4.4754e+00,  1.8790e+00,  1.6390e+00, -2.7539e+00,\n",
      "         2.5508e+00, -3.2321e+00, -2.1154e+00,  2.2940e+00, -1.2243e+00,\n",
      "         6.8939e-01, -3.6389e-01,  2.3071e+00, -1.3735e+00,  5.4604e+00,\n",
      "         1.5212e+00, -1.8205e+00,  2.5466e+00, -1.7257e+00, -1.8663e+00,\n",
      "         6.8231e-01,  5.6992e+00,  2.0273e+00, -4.4387e-01,  3.7821e-01,\n",
      "         3.5337e+00,  5.1913e+00,  1.3576e+00,  7.7780e-01,  1.4515e+00,\n",
      "         4.4637e-02,  5.4579e+00, -2.8163e-01,  2.3233e+00,  4.1817e+00,\n",
      "         3.9538e+00,  5.4703e-01,  2.9619e+00,  2.1917e+00,  2.6100e+00,\n",
      "         3.9760e+00, -2.6037e+00,  2.6961e+00, -2.3917e+00,  1.4010e+00,\n",
      "         1.4465e+00,  1.9770e+00, -2.3206e+00,  4.0960e+00,  8.7336e-01,\n",
      "        -2.5220e-01,  3.1032e+00,  2.4520e+00,  1.1116e+00,  3.4179e-01,\n",
      "         3.5955e+00,  4.3089e-01,  2.5007e+00,  1.5144e+00,  2.5748e+00,\n",
      "         3.0773e+00,  2.3778e+00, -1.2985e+00, -2.2667e+00,  7.5634e-01,\n",
      "        -8.0292e-01,  2.6979e+00, -1.2126e+00,  6.7155e-01,  1.8543e+00,\n",
      "        -3.2609e-01,  1.9189e+00, -1.1188e+00,  4.1289e+00,  3.0058e+00,\n",
      "        -1.2813e+00, -2.2457e+00,  3.0502e+00,  9.7025e-01,  1.7529e+00,\n",
      "         8.6307e-01,  3.9226e+00,  4.4470e-01,  8.5913e-01,  2.8010e+00,\n",
      "        -1.1388e+00,  7.8058e-01,  2.1327e+00,  6.6065e-01,  8.0032e-01,\n",
      "        -2.1195e+00, -1.0734e-01,  3.9187e-01, -9.8508e-01,  2.5286e+00,\n",
      "         1.7073e+00, -1.0204e+00,  1.1539e+00,  8.6578e-01,  1.9766e+00,\n",
      "         1.1121e+00, -4.5241e-02,  1.6311e+00,  7.3761e-01,  1.3037e-01,\n",
      "         3.7855e-01,  4.5436e+00,  1.5055e+00,  3.6920e+00, -1.5581e+00,\n",
      "        -2.6964e+00,  7.0092e-01,  1.4572e-01,  2.3913e+00,  5.1305e-01,\n",
      "        -7.4049e-01, -7.4789e-01,  2.0644e+00, -1.6933e+00, -1.9497e-01,\n",
      "        -3.0840e+00,  1.4701e+00,  5.7413e+00,  1.1369e+00,  1.0061e+00,\n",
      "         3.1480e+00,  2.6179e+00,  7.9771e-01,  7.7618e-02,  7.9875e-01,\n",
      "         2.4599e-01,  1.0249e+00,  1.3173e+00, -1.0443e+00, -1.8979e+00,\n",
      "         1.4719e+00, -1.0078e+00, -3.0942e-01,  1.3479e+00,  1.2727e+00,\n",
      "         2.8599e-01,  3.7255e+00,  3.3852e+00,  1.1846e+00,  1.7428e+00,\n",
      "         5.4140e-01,  1.8737e+00,  3.2574e-01, -9.9135e-01,  5.0324e-01,\n",
      "        -2.2461e+00,  9.2280e-01,  1.4739e+00,  9.7227e-01,  3.4991e+00,\n",
      "         5.0582e-02,  4.2171e+00,  1.3864e-01, -1.2097e+00,  2.4270e+00,\n",
      "         5.0098e-03,  2.5987e+00,  9.9300e-01,  8.9377e-01,  3.8450e-01,\n",
      "         3.2347e+00, -1.3273e+00,  1.3158e+00, -6.3928e-01, -1.0372e+00,\n",
      "         1.6188e-01,  5.2957e+00,  2.6546e+00,  6.8436e-01,  8.0709e-01,\n",
      "         1.5140e+00,  3.6399e+00,  4.3555e+00,  1.4859e+00,  2.6872e+00,\n",
      "        -2.0349e+00, -3.0501e-01, -1.2611e+00, -5.2398e-01,  4.4744e+00,\n",
      "        -1.3190e+00,  4.2012e+00, -2.6686e-01, -2.2140e+00,  8.1486e-01,\n",
      "         5.1380e-01,  4.4472e+00,  1.9871e+00, -4.3114e-01,  1.8861e+00,\n",
      "        -3.5954e-01,  9.9661e-01,  2.0416e-01,  5.3926e-01, -8.7905e-01,\n",
      "         3.5596e-01, -2.1974e+00,  1.6515e+00,  1.7411e+00,  5.0723e+00,\n",
      "         4.9967e+00, -2.9864e+00, -1.5156e+00,  3.5834e-02, -2.5166e+00,\n",
      "         1.7022e+00, -5.5792e-01,  4.7779e-01,  1.3978e+00, -1.6037e+00,\n",
      "        -7.1414e-02, -1.3350e+00,  5.2947e-01,  1.7807e+00, -5.1579e-01,\n",
      "         2.1176e+00,  3.3765e+00,  1.9781e+00,  1.2635e+00,  1.3233e+00,\n",
      "         2.1353e+00,  1.1235e+00,  6.3202e-01,  3.5122e-03,  8.6437e-01,\n",
      "         2.5907e+00,  9.2996e-01,  1.7119e-01,  3.4866e-01, -3.3267e-01,\n",
      "         3.2115e+00,  2.2648e+00,  4.8291e+00, -7.7580e-01,  5.8556e-02,\n",
      "        -2.6401e-01, -2.3351e+00,  2.1407e+00,  1.9448e-01, -4.8841e-01,\n",
      "        -8.9799e-02, -8.6680e-01, -5.9055e-01,  1.9042e-01,  1.6302e+00,\n",
      "        -7.6696e-01, -6.4265e-01,  1.6168e+00,  4.5113e-01,  1.4927e+00,\n",
      "         3.5609e+00,  3.3550e+00,  1.0316e+00,  1.6039e+00,  2.5742e+00,\n",
      "         6.0012e-01,  2.1299e-01,  1.1375e+00, -5.9179e-01,  1.4589e+00,\n",
      "        -7.6012e-01, -4.3384e-01, -1.1908e+00, -1.7915e+00, -9.4198e-01,\n",
      "         4.0434e-01, -1.7250e+00,  4.9512e-01,  1.4894e+00, -6.3255e-02,\n",
      "        -1.2739e+00, -4.5202e-01,  8.7321e-01, -1.0113e-01, -3.4507e+00,\n",
      "        -2.1036e+00, -2.5067e+00,  2.0195e+00,  1.3485e+00, -1.8470e+00,\n",
      "         3.2960e+00,  4.2142e-01,  4.3982e-01, -9.3547e-01, -1.3462e+00,\n",
      "         7.2698e-01,  1.9410e+00,  2.4674e+00,  7.3196e-01,  2.2956e+00],\n",
      "       device='cuda:0')\n",
      "tensor([7.3148e-05, 6.3470e-05, 3.7512e-05, 1.1062e-04, 5.1147e-04, 1.9903e-04,\n",
      "        3.2133e-04, 6.1659e-05, 3.5347e-05, 6.3279e-07, 4.5231e-06, 2.4884e-06,\n",
      "        1.8210e-05, 4.0306e-06, 1.8079e-06, 2.4994e-06, 7.4366e-06, 2.5466e-06,\n",
      "        1.6563e-06, 5.1006e-06, 1.2831e-06, 4.4691e-07, 3.6859e-07, 5.5340e-06,\n",
      "        1.3010e-07, 1.0402e-04, 2.7141e-05, 3.5448e-05, 3.7753e-04, 3.6749e-04,\n",
      "        4.2096e-05, 2.2882e-05, 5.3197e-05, 9.4381e-05, 1.2925e-04, 8.2164e-05,\n",
      "        4.5635e-05, 6.9241e-05, 1.4545e-05, 6.2994e-06, 4.6068e-06, 6.1709e-06,\n",
      "        2.7454e-06, 2.5181e-05, 2.2505e-05, 4.5373e-05, 4.2501e-06, 1.2386e-05,\n",
      "        1.4193e-05, 1.6884e-05, 6.7467e-06, 4.2611e-03, 4.4537e-05, 1.9030e-05,\n",
      "        2.0636e-05, 2.5275e-06, 2.1708e-05, 8.4706e-06, 8.5868e-06, 9.7901e-06,\n",
      "        1.0286e-04, 7.5254e-05, 3.2923e-05, 5.1746e-05, 2.7678e-05, 1.5307e-05,\n",
      "        4.4701e-05, 2.0136e-05, 1.2413e-05, 1.0011e-05, 2.5736e-06, 3.6063e-04,\n",
      "        2.6222e-06, 2.9261e-05, 4.9433e-06, 7.3255e-05, 1.1542e-04, 1.7570e-05,\n",
      "        6.1937e-04, 9.7431e-05, 6.7340e-06, 1.3811e-06, 3.9993e-06, 3.8591e-06,\n",
      "        1.0192e-06, 6.2601e-06, 6.0085e-06, 2.7862e-05, 4.7618e-06, 3.4064e-05,\n",
      "        1.4899e-06, 2.9174e-06, 3.6554e-06, 5.4733e-06, 2.0633e-06, 8.5440e-07,\n",
      "        1.7571e-05, 7.1717e-06, 5.5959e-07, 1.3686e-05, 9.7403e-07, 1.3996e-05,\n",
      "        1.6712e-04, 2.4167e-04, 8.9358e-05, 2.5106e-05, 7.9846e-05, 4.7700e-05,\n",
      "        6.1639e-05, 3.0842e-06, 4.9605e-06, 1.1666e-05, 1.3630e-03, 3.4108e-04,\n",
      "        1.9553e-04, 1.8797e-05, 6.1793e-06, 1.2078e-04, 2.4830e-04, 1.1894e-05,\n",
      "        1.3317e-05, 1.1464e-04, 1.7871e-04, 1.2690e-05, 8.6482e-05, 3.6732e-04,\n",
      "        1.0623e-04, 4.0377e-07, 1.1022e-07, 1.8376e-06, 3.6991e-06, 2.1081e-07,\n",
      "        8.6061e-07, 1.6729e-07, 9.4548e-07, 1.0246e-06, 2.5876e-07, 3.1432e-06,\n",
      "        3.1029e-07, 5.2161e-07, 8.5913e-07, 1.8653e-07, 1.8196e-07, 6.6480e-07,\n",
      "        1.2259e-05, 1.7643e-05, 4.0344e-06, 3.2401e-07, 1.2236e-05, 3.3541e-05,\n",
      "        7.1536e-05, 1.3275e-03, 1.7870e-05, 1.4847e-04, 5.6849e-04, 1.5448e-04,\n",
      "        3.4456e-04, 5.7456e-05, 1.4125e-04, 5.2933e-04, 6.8476e-05, 1.0835e-03,\n",
      "        7.5968e-04, 1.6351e-03, 2.9852e-05, 5.5845e-05, 3.2533e-05, 4.9874e-05,\n",
      "        1.0873e-03, 7.1487e-05, 5.4453e-05, 6.1809e-04, 3.3356e-04, 6.7720e-05,\n",
      "        2.2272e-04, 1.8371e-05, 3.6520e-05, 1.4565e-05, 4.2823e-04, 7.9583e-04,\n",
      "        4.8940e-04, 2.1563e-05, 2.5264e-04, 1.7657e-05, 8.2107e-04, 1.0922e-03,\n",
      "        4.2689e-04, 6.5336e-05, 2.7160e-05, 3.2114e-04, 9.0204e-05, 1.4186e-04,\n",
      "        2.9465e-04, 1.3358e-04, 1.1277e-04, 3.9198e-04, 8.1309e-05, 1.4408e-04,\n",
      "        3.9112e-05, 6.7107e-04, 6.9719e-05, 2.5968e-04, 5.1100e-04, 6.5273e-04,\n",
      "        5.0913e-04, 5.0070e-05, 3.3059e-05, 1.9231e-03, 3.4303e-03, 2.1046e-03,\n",
      "        8.4493e-05, 3.7005e-04, 6.4226e-05, 1.8453e-04, 3.5095e-05, 1.1538e-04,\n",
      "        1.1447e-04, 1.0369e-04, 9.8804e-05, 3.4872e-04, 3.8289e-04, 1.0071e-04,\n",
      "        3.0588e-04, 6.5608e-03, 1.3861e-04, 5.2971e-04, 1.0030e-04, 2.1003e-03,\n",
      "        1.0001e-04, 3.5338e-04, 4.2897e-04, 1.0873e-04, 3.7802e-05, 5.6799e-05,\n",
      "        4.0586e-04, 2.3141e-04, 5.5263e-04, 4.7033e-04, 9.6879e-05, 4.0318e-04,\n",
      "        7.1728e-05, 1.5975e-04, 5.7174e-04, 5.0051e-04, 3.6915e-05, 1.9369e-03,\n",
      "        1.1310e-04, 2.1741e-04, 2.0136e-04, 5.4392e-04, 6.7533e-04, 5.0724e-04,\n",
      "        6.3845e-05, 3.2121e-04, 3.5507e-04, 1.9507e-05, 2.1417e-04, 1.0126e-03,\n",
      "        1.6026e-03, 2.2541e-03, 1.2077e-03, 9.6853e-05, 1.8050e-04, 2.9804e-03,\n",
      "        1.4789e-03, 3.0333e-04, 1.2186e-04, 1.0492e-04, 9.4363e-05, 4.6549e-06,\n",
      "        1.7944e-05, 1.6926e-05, 2.1051e-06, 6.5483e-04, 1.4537e-05, 6.4981e-06,\n",
      "        1.6943e-05, 1.7186e-05, 6.2148e-06, 1.7512e-05, 9.9824e-06, 1.0345e-04,\n",
      "        3.6280e-05, 3.0094e-04, 2.7225e-03, 1.0231e-04, 4.8480e-05, 2.0129e-05,\n",
      "        2.8729e-06, 7.5930e-07, 5.2689e-06, 1.0026e-05, 8.9557e-06, 2.1448e-06,\n",
      "        5.5701e-06, 1.6454e-06, 1.8202e-05, 5.5886e-06, 3.2263e-05, 4.7914e-05,\n",
      "        5.7359e-06, 8.0917e-05, 1.3581e-04, 7.1234e-06, 1.4887e-04, 5.9056e-04,\n",
      "        3.9452e-04, 1.3082e-05, 4.2048e-06, 1.9177e-05, 5.8925e-05, 3.9812e-05,\n",
      "        1.1064e-04, 8.5149e-06, 3.1570e-03, 1.7156e-05, 1.8748e-06, 2.4759e-06,\n",
      "        3.5812e-06, 1.1400e-06, 2.0285e-06, 3.1123e-06, 8.5062e-06, 8.4700e-06,\n",
      "        1.1193e-05, 2.9352e-05, 1.0158e-05, 2.7669e-05, 1.3765e-05, 5.3604e-06,\n",
      "        4.2192e-05, 6.4979e-06, 5.6857e-04, 4.2057e-05, 3.0935e-05, 7.4659e-07,\n",
      "        3.7263e-06, 1.6955e-05, 1.8681e-04, 5.3874e-05, 3.1617e-06, 2.7719e-04,\n",
      "        5.2720e-05, 4.3801e-05, 1.9030e-05, 1.7383e-03, 1.4203e-04, 2.0364e-04,\n",
      "        6.3623e-05, 4.2574e-05, 5.2806e-05, 6.1820e-05, 3.5480e-05, 3.2062e-05,\n",
      "        8.6018e-05, 2.9556e-05, 8.2833e-05, 1.1006e-05, 8.4100e-05, 2.6654e-04,\n",
      "        1.0494e-05, 7.5998e-04, 2.5241e-04, 3.0911e-04, 1.1316e-06, 5.1283e-06,\n",
      "        1.8302e-06, 1.3214e-06, 3.0717e-06, 5.2686e-07, 2.6211e-06, 8.0683e-06,\n",
      "        4.4589e-06, 4.2423e-05, 3.5542e-05, 1.0644e-06, 4.9184e-06, 4.3661e-06,\n",
      "        2.1722e-05, 3.1332e-06, 2.0926e-05, 2.8361e-06, 9.6936e-06, 1.1470e-05,\n",
      "        1.2775e-05, 2.8575e-05, 3.9449e-05, 6.1523e-05, 6.9178e-05, 8.4906e-06,\n",
      "        7.5443e-06, 2.5233e-05, 9.6441e-06, 5.4559e-05, 1.2765e-04, 7.2190e-06,\n",
      "        8.9823e-06, 4.6744e-06, 3.2401e-04, 7.0808e-05, 5.2174e-05, 3.6935e-04,\n",
      "        4.2278e-03, 1.1704e-04, 9.9917e-05, 4.9337e-04, 2.1152e-04, 1.8122e-03,\n",
      "        6.3632e-04, 1.2001e-04, 8.8614e-05, 6.7698e-05, 2.8266e-03, 7.0630e-04,\n",
      "        8.1259e-05, 6.5469e-04, 7.4009e-05, 1.5468e-04, 6.5594e-06, 3.4639e-04,\n",
      "        2.7982e-03, 4.9788e-04, 2.9867e-03, 4.0426e-03, 1.5489e-04, 1.9930e-05,\n",
      "        2.8987e-04, 4.5761e-03, 4.6439e-03, 2.2907e-04, 3.3934e-05, 5.0293e-04,\n",
      "        1.5274e-04, 1.9958e-04, 1.8850e-05, 1.0137e-02, 1.7339e-03, 2.8084e-04,\n",
      "        1.5155e-05, 7.9228e-05, 3.5303e-04, 1.5277e-04, 4.6954e-04, 5.5785e-06,\n",
      "        9.4472e-05, 7.4912e-05, 3.3537e-05, 3.1711e-04, 2.8223e-04, 9.5728e-05,\n",
      "        1.3495e-03, 1.7992e-05, 1.0802e-04, 5.5596e-04, 3.8652e-05, 1.2615e-04,\n",
      "        1.5750e-04, 3.5489e-05, 7.3397e-06, 7.0512e-05, 3.9319e-05, 2.8209e-04,\n",
      "        2.5410e-04, 3.7480e-04, 3.0852e-05, 9.2251e-05, 3.4826e-04, 5.3739e-04,\n",
      "        8.8807e-04, 1.2773e-02, 1.5045e-03, 5.1918e-03, 1.0422e-04, 2.1400e-03,\n",
      "        1.7241e-05, 1.9388e-05, 1.3824e-03, 7.1645e-05, 2.6318e-03, 8.2363e-04,\n",
      "        8.1951e-05, 5.8925e-05, 9.7456e-05, 7.7525e-05, 2.1105e-05, 3.3563e-04,\n",
      "        1.6668e-02, 2.0503e-05, 1.0224e-04, 1.4356e-05, 3.6674e-05, 8.7450e-04,\n",
      "        5.6365e-03, 1.6960e-03, 6.4856e-04, 8.7994e-04, 1.1317e-04, 2.0722e-04,\n",
      "        3.1281e-04, 4.6725e-03, 2.4309e-04, 5.1814e-05, 1.1192e-02, 3.2854e-05,\n",
      "        3.3477e-04, 2.5948e-03, 4.9145e-04, 1.9585e-04, 8.1206e-04, 8.3281e-05,\n",
      "        1.8405e-05, 4.4882e-04, 1.2608e-03, 1.8504e-04, 7.9913e-04, 1.6507e-03,\n",
      "        8.6149e-03, 6.8020e-04, 1.6040e-03, 2.5622e-03, 5.1430e-04, 1.0096e-02,\n",
      "        3.1134e-05, 9.6899e-04, 2.8865e-04, 2.4760e-04, 1.5315e-02, 6.6403e-04,\n",
      "        1.3892e-03, 3.5374e-05, 2.5060e-04, 1.3476e-04, 5.7051e-02, 8.7791e-06,\n",
      "        6.0585e-03, 2.0177e-03, 5.4365e-05, 1.0872e-05, 5.3324e-04, 3.4575e-04,\n",
      "        1.9814e-04, 7.8892e-03, 9.2291e-04, 2.2448e-03, 9.6241e-04, 1.0093e-03,\n",
      "        3.3290e-03, 2.7502e-05, 7.1685e-05, 3.4774e-05, 7.3862e-04, 1.7408e-05,\n",
      "        4.4835e-04, 2.2511e-03, 6.3917e-06, 1.3970e-04, 1.1844e-03, 2.4399e-05,\n",
      "        3.4648e-04, 2.5544e-03, 9.3987e-05, 9.0173e-04, 5.7012e-05, 1.2671e-05,\n",
      "        4.6212e-04, 7.3817e-05, 7.3673e-05, 1.4010e-03, 6.1348e-05, 8.6077e-04,\n",
      "        5.6443e-04, 1.2666e-04, 5.7879e-05, 4.8207e-05, 4.7344e-05, 6.9336e-03,\n",
      "        2.5046e-05, 1.3233e-03, 1.2800e-04, 1.7577e-02, 2.9289e-04, 1.0554e-04,\n",
      "        4.5174e-04, 1.2543e-03, 5.7227e-05, 2.4828e-05, 2.6282e-03, 1.8239e-03,\n",
      "        3.2538e-04, 1.8615e-03, 1.9264e-05, 5.3142e-05, 1.1985e-04, 1.0533e-04,\n",
      "        1.7449e-03, 2.9598e-04, 1.8226e-04, 5.0786e-04, 1.7830e-03, 1.4355e-04,\n",
      "        2.3263e-04, 5.6410e-05, 9.5887e-06, 2.5429e-04, 3.3766e-04, 4.0070e-05,\n",
      "        3.0449e-03, 1.3007e-03, 3.5917e-05, 5.7736e-03, 2.5699e-05, 1.4006e-03,\n",
      "        1.6639e-05, 1.3278e-03, 7.1686e-05, 6.2403e-04, 1.6875e-05, 2.5077e-04,\n",
      "        2.8835e-03, 6.8347e-04, 1.4168e-03, 1.7136e-03, 2.8415e-05, 2.2807e-05,\n",
      "        7.9983e-04, 4.2001e-05, 9.1863e-05, 1.7959e-03, 1.1558e-04, 4.6017e-05,\n",
      "        1.1304e-03, 4.7866e-05, 4.4641e-04, 9.4962e-05, 2.7172e-03, 1.4070e-04,\n",
      "        4.2828e-05, 2.8134e-04, 8.3653e-05, 6.1828e-05, 3.0340e-05, 9.6525e-04,\n",
      "        4.5236e-03, 1.4964e-04, 1.4306e-04, 7.4815e-06, 7.3276e-04, 1.8326e-04,\n",
      "        4.3730e-04, 1.5371e-05, 4.6932e-05, 1.3669e-03, 2.9758e-04, 7.4525e-03,\n",
      "        2.0706e-03, 2.2862e-04, 2.8407e-04, 2.6036e-03, 1.1035e-05, 4.8215e-05,\n",
      "        3.6991e-04, 7.4563e-04, 2.1326e-04, 1.5547e-04, 2.3152e-03, 9.5366e-03,\n",
      "        2.9463e-03, 1.7564e-04, 3.8861e-04, 1.4165e-04, 1.5975e-02, 1.0965e-04,\n",
      "        2.4299e-04, 4.7170e-04, 8.4353e-03, 4.0640e-03, 1.9083e-04, 9.3339e-05,\n",
      "        6.0987e-04, 2.8617e-05, 1.1329e-04, 5.0215e-04, 2.4078e-04, 3.3774e-04,\n",
      "        3.2956e-04, 1.1653e-05, 9.7927e-05, 6.7985e-04, 2.2592e-04, 3.3646e-05,\n",
      "        2.0140e-03, 4.3996e-04, 3.9776e-05, 6.7704e-05, 2.8946e-05, 1.8884e-04,\n",
      "        3.8226e-04, 4.1278e-04, 3.9002e-04, 4.1370e-04, 3.8883e-03, 1.8572e-05,\n",
      "        1.4051e-05, 2.6207e-04, 1.3296e-04, 1.0676e-04, 1.0366e-02, 7.7263e-04,\n",
      "        6.0781e-04, 7.5150e-06, 1.5127e-03, 4.6588e-06, 1.4231e-05, 1.1701e-03,\n",
      "        3.4693e-05, 2.3515e-04, 8.2019e-05, 1.1855e-03, 2.9885e-05, 2.7758e-02,\n",
      "        5.4023e-04, 1.9112e-05, 1.5063e-03, 2.1012e-05, 1.8257e-05, 2.3349e-04,\n",
      "        3.5245e-02, 8.9617e-04, 7.5715e-05, 1.7227e-04, 4.0422e-03, 2.1208e-02,\n",
      "        4.5874e-04, 2.5689e-04, 5.0386e-04, 1.2341e-04, 2.7688e-02, 8.9051e-05,\n",
      "        1.2048e-03, 7.7272e-03, 6.1524e-03, 2.0395e-04, 2.2818e-03, 1.0564e-03,\n",
      "        1.6049e-03, 6.2910e-03, 8.7332e-06, 1.7493e-03, 1.0795e-05, 4.7906e-04,\n",
      "        5.0139e-04, 8.5222e-04, 1.1591e-05, 7.0930e-03, 2.8265e-04, 9.1711e-05,\n",
      "        2.6281e-03, 1.3704e-03, 3.5869e-04, 1.6611e-04, 4.3000e-03, 1.8159e-04,\n",
      "        1.4388e-03, 5.3659e-04, 1.5494e-03, 2.5610e-03, 1.2724e-03, 3.2213e-05,\n",
      "        1.2233e-05, 2.5143e-04, 5.2874e-05, 1.7523e-03, 3.5100e-05, 2.3099e-04,\n",
      "        7.5381e-04, 8.5178e-05, 8.0410e-04, 3.8552e-05, 7.3302e-03, 2.3843e-03,\n",
      "        3.2771e-05, 1.2492e-05, 2.4926e-03, 3.1140e-04, 6.8114e-04, 2.7975e-04,\n",
      "        5.9636e-03, 1.8411e-04, 2.7865e-04, 1.9426e-03, 3.7791e-05, 2.5760e-04,\n",
      "        9.9575e-04, 2.2849e-04, 2.6274e-04, 1.4172e-05, 1.0601e-04, 1.7464e-04,\n",
      "        4.4069e-05, 1.4795e-03, 6.5077e-04, 4.2539e-05, 3.7418e-04, 2.8051e-04,\n",
      "        8.5187e-04, 3.5885e-04, 1.1280e-04, 6.0301e-04, 2.4677e-04, 1.3445e-04,\n",
      "        1.7233e-04, 1.1097e-02, 5.3181e-04, 4.7357e-03, 2.4848e-05, 7.9605e-06,\n",
      "        2.3788e-04, 1.3653e-04, 1.2897e-03, 1.9714e-04, 5.6280e-05, 5.5865e-05,\n",
      "        9.3004e-04, 2.1705e-05, 9.7112e-05, 5.4023e-06, 5.1334e-04, 3.6760e-02,\n",
      "        3.6789e-04, 3.2277e-04, 2.7486e-03, 1.6177e-03, 2.6205e-04, 1.2754e-04,\n",
      "        2.6233e-04, 1.5093e-04, 3.2888e-04, 4.4062e-04, 4.1537e-05, 1.7689e-05,\n",
      "        5.1427e-04, 4.3078e-05, 8.6610e-05, 4.5431e-04, 4.2139e-04, 1.5709e-04,\n",
      "        4.8970e-03, 3.4845e-03, 3.8585e-04, 6.7429e-04, 2.0280e-04, 7.6861e-04,\n",
      "        1.6346e-04, 4.3794e-05, 1.9521e-04, 1.2487e-05, 2.9697e-04, 5.1529e-04,\n",
      "        3.1203e-04, 3.9046e-03, 1.2414e-04, 8.0063e-03, 1.3557e-04, 3.5202e-05,\n",
      "        1.3365e-03, 1.1861e-04, 1.5869e-03, 3.1857e-04, 2.8847e-04, 1.7335e-04,\n",
      "        2.9974e-03, 3.1298e-05, 4.3994e-04, 6.2275e-05, 4.1830e-05, 1.3876e-04,\n",
      "        2.3542e-02, 1.6781e-03, 2.3397e-04, 2.6452e-04, 5.3640e-04, 4.4951e-03,\n",
      "        9.1944e-03, 5.2149e-04, 1.7338e-03, 1.5424e-05, 8.6993e-05, 3.3441e-05,\n",
      "        6.9886e-05, 1.0355e-02, 3.1558e-05, 7.8795e-03, 9.0376e-05, 1.2895e-05,\n",
      "        2.6659e-04, 1.9728e-04, 1.0077e-02, 8.6086e-04, 7.6684e-05, 7.7813e-04,\n",
      "        8.2377e-05, 3.1972e-04, 1.4475e-04, 2.0237e-04, 4.8998e-05, 1.6848e-04,\n",
      "        1.3111e-05, 6.1545e-04, 6.7312e-04, 1.8828e-02, 1.7459e-02, 5.9565e-06,\n",
      "        2.5925e-05, 1.2232e-04, 9.5283e-06, 6.4747e-04, 6.7553e-05, 1.9031e-04,\n",
      "        4.7756e-04, 2.3739e-05, 1.0988e-04, 3.1059e-05, 2.0040e-04, 7.0032e-04,\n",
      "        7.0460e-05, 9.8082e-04, 3.4542e-03, 8.5316e-04, 4.1752e-04, 4.4323e-04,\n",
      "        9.9836e-04, 3.6299e-04, 2.2204e-04, 1.1843e-04, 2.8012e-04, 1.5743e-03,\n",
      "        2.9911e-04, 1.4005e-04, 1.6725e-04, 8.4620e-05, 2.9288e-03, 1.1364e-03,\n",
      "        1.4763e-02, 5.4328e-05, 1.2514e-04, 9.0634e-05, 1.1424e-05, 1.0038e-03,\n",
      "        1.4335e-04, 7.2416e-05, 1.0788e-04, 4.9602e-05, 6.5385e-05, 1.4277e-04,\n",
      "        6.0248e-04, 5.4810e-05, 6.2065e-05, 5.9446e-04, 1.8530e-04, 5.2509e-04,\n",
      "        4.1537e-03, 3.3808e-03, 3.3111e-04, 5.8683e-04, 1.5484e-03, 2.1507e-04,\n",
      "        1.4603e-04, 3.6811e-04, 6.5304e-05, 5.0762e-04, 5.5186e-05, 7.6477e-05,\n",
      "        3.5873e-05, 1.9674e-05, 4.6010e-05, 1.7683e-04, 2.1027e-05, 1.9363e-04,\n",
      "        5.2332e-04, 1.1078e-04, 3.3015e-05, 7.5100e-05, 2.8260e-04, 1.0667e-04,\n",
      "        3.7440e-06, 1.4400e-05, 9.6228e-06, 8.8920e-04, 4.5456e-04, 1.8612e-05,\n",
      "        3.1870e-03, 1.7987e-04, 1.8322e-04, 4.6311e-05, 3.0712e-05, 2.4416e-04,\n",
      "        8.2208e-04, 1.3917e-03, 2.4538e-04, 1.1720e-03], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad(): #Disabling gradient calculation when not calling tensor.backward()\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0]) #output = confidence score for each class\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #because only has 1 image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-08 23:15:34--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10472 (10K) [text/plain]\n",
      "Saving to: ‘imagenet_classes.txt’\n",
      "\n",
      "imagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-10-08 23:15:34 (143 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dining table 0.057050980627536774\n",
      "stove 0.03675980120897293\n",
      "plane 0.03524487838149071\n",
      "piggy bank 0.027757996693253517\n",
      "pool table 0.02768792025744915\n"
     ]
    }
   ],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
